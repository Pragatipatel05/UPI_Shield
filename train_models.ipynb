{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ee1b54f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import os\n",
    "from collections import Counter\n",
    "from math import log, exp\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "# dataset + split + scaler\n",
    "def generate_or_load_dataset(path='upi_fraud_dataset.csv', n_samples=3000, seed=42):\n",
    "    if os.path.exists(path):\n",
    "        df = pd.read_csv(path)\n",
    "        print(\"Loaded dataset:\", df.shape)\n",
    "        return df\n",
    "    np.random.seed(seed)\n",
    "    data = {\n",
    "        'trans_hour': np.random.randint(0, 24, n_samples),\n",
    "        'trans_day': np.random.randint(1, 32, n_samples),\n",
    "        'trans_month': np.random.randint(1, 13, n_samples),\n",
    "        'trans_year': np.random.choice([2022, 2023], n_samples),\n",
    "        'category': np.random.randint(0, 15, n_samples),\n",
    "        'upi_number': np.random.randint(9000000000, 9999999999, n_samples),\n",
    "        'age': np.random.randint(18, 80, n_samples),\n",
    "        'trans_amount': np.random.exponential(1000, n_samples),\n",
    "        'state': np.random.randint(1, 36, n_samples),\n",
    "        'zip': np.random.randint(100000, 999999, n_samples),\n",
    "    }\n",
    "    fraud_prob = (\n",
    "        (data['trans_hour'] < 6) * 0.3 +\n",
    "        (data['trans_amount'] > 5000) * 0.4 +\n",
    "        (data['age'] < 25) * 0.2 +\n",
    "        np.random.random(n_samples) * 0.3\n",
    "    )\n",
    "    data['fraud_risk'] = (fraud_prob > 0.6).astype(int)\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(path, index=False)\n",
    "    print(\"Generated dataset:\", df.shape)\n",
    "    return df\n",
    "\n",
    "def train_test_split_np(X, y, test_size=0.2, seed=42):\n",
    "    np.random.seed(seed)\n",
    "    n = X.shape[0]\n",
    "    idx = np.random.permutation(n)\n",
    "    test_n = int(n * test_size)\n",
    "    return X[idx[test_n:]], X[idx[:test_n]], y[idx[test_n:]], y[idx[:test_n]]\n",
    "\n",
    "class StandardScalerManual:\n",
    "    def fit(self, X):\n",
    "        self.mean_ = np.mean(X, axis=0)\n",
    "        self.scale_ = np.std(X, axis=0, ddof=0)\n",
    "        self.scale_[self.scale_ == 0] = 1.0\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return (X - self.mean_) / self.scale_\n",
    "    def fit_transform(self, X):\n",
    "        return self.fit(X).transform(X)\n",
    "    def save(self, fname):\n",
    "        joblib.dump({'mean': self.mean_, 'scale': self.scale_}, fname)\n",
    "    @classmethod\n",
    "    def load(cls, fname):\n",
    "        data = joblib.load(fname)\n",
    "        s = cls()\n",
    "        s.mean_ = data['mean']\n",
    "        s.scale_ = data['scale']\n",
    "        return s\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8f5b27be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "class LogisticRegressionSGD:\n",
    "    def __init__(self, lr=0.02, n_iter=2000, l2=0.0, verbose=False):\n",
    "        self.lr = lr; self.n_iter = n_iter; self.l2 = l2; self.verbose = verbose\n",
    "    def _sigmoid(self, z):\n",
    "        z = np.clip(z, -500, 500)\n",
    "        return 1.0 / (1.0 + np.exp(-z))\n",
    "    def fit(self, X, y):\n",
    "        n, d = X.shape\n",
    "        self.w = np.zeros(d)\n",
    "        self.b = 0.0\n",
    "        for it in range(self.n_iter):\n",
    "            z = X.dot(self.w) + self.b\n",
    "            preds = self._sigmoid(z)\n",
    "            error = preds - y\n",
    "            gw = (X.T.dot(error)) / n + self.l2 * self.w\n",
    "            gb = np.sum(error) / n\n",
    "            self.w -= self.lr * gw\n",
    "            self.b -= self.lr * gb\n",
    "            if self.verbose and it % (self.n_iter//5+1) == 0:\n",
    "                loss = -np.mean(y*np.log(preds+1e-12) + (1-y)*np.log(1-preds+1e-12))\n",
    "                print(f\"[LogReg] iter {it} loss {loss:.4f}\")\n",
    "        return self\n",
    "    def predict_proba(self, X):\n",
    "        p = self._sigmoid(X.dot(self.w) + self.b)\n",
    "        return np.vstack([1-p, p]).T\n",
    "    def predict(self, X, thr=0.5):\n",
    "        return (self.predict_proba(X)[:,1] >= thr).astype(int)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "419f56fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM\n",
    "class LinearSVM_SGD:\n",
    "    def __init__(self, lr=0.005, n_iter=2000, C=1.0, verbose=False):\n",
    "        self.lr = lr; self.n_iter = n_iter; self.C = C; self.verbose = verbose\n",
    "    def fit(self, X, y):\n",
    "        # y in {0,1} -> {-1,+1}\n",
    "        y_ = (y*2 - 1).astype(float)\n",
    "        n,d = X.shape\n",
    "        self.w = np.zeros(d)\n",
    "        self.b = 0.0\n",
    "        for it in range(self.n_iter):\n",
    "            margins = y_ * (X.dot(self.w) + self.b)\n",
    "            idx = margins < 1\n",
    "            if idx.any():\n",
    "                grad_w = self.w - (self.C/n) * (X[idx].T.dot(y_[idx]))\n",
    "                grad_b = - (self.C/n) * np.sum(y_[idx])\n",
    "            else:\n",
    "                grad_w = self.w\n",
    "                grad_b = 0.0\n",
    "            self.w -= self.lr * grad_w\n",
    "            self.b -= self.lr * grad_b\n",
    "            if self.verbose and it % (self.n_iter//5+1) == 0:\n",
    "                loss = 0.5*np.dot(self.w,self.w) + self.C*np.mean(np.maximum(0,1-margins))\n",
    "                print(f\"[SVM] iter {it} loss {loss:.4f}\")\n",
    "        return self\n",
    "    def decision_function(self, X): return X.dot(self.w) + self.b\n",
    "    def predict(self, X): return (self.decision_function(X) >= 0).astype(int)\n",
    "    def predict_proba(self, X):\n",
    "        s = self.decision_function(X)\n",
    "        p = 1.0/(1.0+np.exp(-s))\n",
    "        return np.vstack([1-p,p]).T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fb7e6c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree  + Random Forest\n",
    "def gini(y):\n",
    "    if len(y)==0: return 0.0\n",
    "    ps = np.bincount(y)/len(y)\n",
    "    return 1.0 - np.sum(ps*ps)\n",
    "\n",
    "class DecisionTreeNode:\n",
    "    def __init__(self, feature=None, threshold=None, left=None, right=None, value=None):\n",
    "        self.feature=feature; self.threshold=threshold; self.left=left; self.right=right; self.value=value\n",
    "\n",
    "class DecisionTree:\n",
    "    def __init__(self, max_depth=10, min_samples_split=2, max_features=None):\n",
    "        self.max_depth=max_depth; self.min_samples_split=min_samples_split; self.max_features=max_features\n",
    "    def fit(self, X, y):\n",
    "        self.n_features_ = X.shape[1]\n",
    "        self.root = self._grow_tree(X,y,0)\n",
    "    def _best_split(self, X, y):\n",
    "        n,d = X.shape\n",
    "        features = range(d) if self.max_features is None else np.random.choice(d, self.max_features, replace=False)\n",
    "        best=None; best_gain=0.0\n",
    "        parent = gini(y)\n",
    "        for feat in features:\n",
    "            vals = X[:,feat]; uniq = np.unique(vals)\n",
    "            if uniq.shape[0]==1: continue\n",
    "            thr_candidates = (uniq[:-1]+uniq[1:])/2.0\n",
    "            for thr in thr_candidates:\n",
    "                left_idx = vals <= thr; right_idx = ~left_idx\n",
    "                if left_idx.sum()==0 or right_idx.sum()==0: continue\n",
    "                g_left = gini(y[left_idx]); g_right = gini(y[right_idx])\n",
    "                gain = parent - (left_idx.sum()/n)*g_left - (right_idx.sum()/n)*g_right\n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    best = (feat, thr, left_idx, right_idx)\n",
    "        return best, best_gain\n",
    "    def _grow_tree(self, X, y, depth):\n",
    "        n = len(y)\n",
    "        num_pos = np.sum(y==1)\n",
    "        if depth >= self.max_depth or n < self.min_samples_split or num_pos==0 or num_pos==n:\n",
    "            return DecisionTreeNode(value=int(round(np.mean(y))))\n",
    "        split, gain = self._best_split(X,y)\n",
    "        if split is None or gain<=0:\n",
    "            return DecisionTreeNode(value=int(round(np.mean(y))))\n",
    "        feat, thr, left_idx, right_idx = split\n",
    "        left = self._grow_tree(X[left_idx], y[left_idx], depth+1)\n",
    "        right = self._grow_tree(X[right_idx], y[right_idx], depth+1)\n",
    "        return DecisionTreeNode(feature=feat, threshold=thr, left=left, right=right)\n",
    "    def _predict_one(self, x, node):\n",
    "        if node.value is not None: return node.value\n",
    "        if x[node.feature] <= node.threshold:\n",
    "            return self._predict_one(x, node.left)\n",
    "        else:\n",
    "            return self._predict_one(x, node.right)\n",
    "    def predict(self, X):\n",
    "        return np.array([self._predict_one(x, self.root) for x in X])\n",
    "    def predict_proba(self, X):\n",
    "        p = self.predict(X)\n",
    "        return np.vstack([1-p, p]).T\n",
    "\n",
    "class RandomForest:\n",
    "    def __init__(self, n_estimators=100, max_depth=12, min_samples_split=5, max_features=None, seed=42):\n",
    "        self.n_estimators=n_estimators; self.max_depth=max_depth; self.min_samples_split=min_samples_split\n",
    "        self.max_features=max_features; self.seed=seed\n",
    "    def fit(self, X, y):\n",
    "        np.random.seed(self.seed)\n",
    "        self.trees=[]\n",
    "        n=X.shape[0]\n",
    "        for i in range(self.n_estimators):\n",
    "            idx = np.random.choice(n, n, replace=True)\n",
    "            Xb = X[idx]; yb = y[idx]\n",
    "            tree = DecisionTree(max_depth=self.max_depth, min_samples_split=self.min_samples_split, max_features=self.max_features)\n",
    "            tree.fit(Xb, yb)\n",
    "            self.trees.append(tree)\n",
    "    def predict_proba(self, X):\n",
    "        probs = np.array([t.predict_proba(X)[:,1] for t in self.trees])  # shape (n_trees, n_samples)\n",
    "        avg = np.mean(probs, axis=0)\n",
    "        return np.vstack([1-avg, avg]).T\n",
    "    def predict(self, X):\n",
    "        return (self.predict_proba(X)[:,1] >= 0.5).astype(int)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "584355d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Boosting\n",
    "class GradientBoostingStumps:\n",
    "    def __init__(self, n_estimators=50, learning_rate=0.1):\n",
    "        self.n_estimators=n_estimators; self.lr=learning_rate\n",
    "    def fit(self, X, y):\n",
    "        n,d = X.shape\n",
    "        p = np.clip(np.mean(y), 1e-6, 1-1e-6)\n",
    "        F0 = 0.5 * np.log(p/(1-p))\n",
    "        self.base_score = F0\n",
    "        self.trees=[]; self.scalars=[]\n",
    "        F = np.full(n, F0)\n",
    "        for m in range(self.n_estimators):\n",
    "            p = 1.0/(1.0+np.exp(-F))\n",
    "            residual = y - p\n",
    "            best=None; best_loss=float('inf')\n",
    "            for feat in range(d):\n",
    "                vals = X[:,feat]; uniq = np.unique(vals)\n",
    "                if uniq.shape[0]==1: continue\n",
    "                thr_cands = (uniq[:-1]+uniq[1:])/2.0\n",
    "                for thr in thr_cands:\n",
    "                    left = vals <= thr; right = ~left\n",
    "                    if left.sum()==0 or right.sum()==0: continue\n",
    "                    left_mean = residual[left].mean(); right_mean = residual[right].mean()\n",
    "                    loss = ((residual[left]-left_mean)**2).sum() + ((residual[right]-right_mean)**2).sum()\n",
    "                    if loss < best_loss:\n",
    "                        best_loss = loss\n",
    "                        best = (feat, thr, left_mean, right_mean)\n",
    "            if best is None: break\n",
    "            self.trees.append(best)\n",
    "            self.scalars.append(1.0)\n",
    "            feat, thr, lm, rm = best\n",
    "            F += self.lr * (np.where(X[:,feat] <= thr, lm, rm))\n",
    "        return self\n",
    "    def predict_proba(self, X):\n",
    "        n = X.shape[0]\n",
    "        F = np.full(n, self.base_score)\n",
    "        for (feat,thr,lm,rm), s in zip(self.trees, self.scalars):\n",
    "            F += self.lr * (np.where(X[:,feat] <= thr, lm, rm) * s)\n",
    "        p = 1.0/(1.0+np.exp(-F))\n",
    "        return np.vstack([1-p, p]).T\n",
    "    def predict(self, X):\n",
    "        return (self.predict_proba(X)[:,1] >= 0.5).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5a9947d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP (NumPy) â€” dense network\n",
    "def relu(x): return np.maximum(0, x)\n",
    "def drelu(x): return (x > 0).astype(float)\n",
    "def softmax(z):\n",
    "    z = z - np.max(z, axis=1, keepdims=True)\n",
    "    ez = np.exp(z)\n",
    "    return ez / np.sum(ez, axis=1, keepdims=True)\n",
    "\n",
    "class MLP_Numpy:\n",
    "    def __init__(self, layer_sizes, lr=0.005, n_epochs=500, batch_size=42, verbose=True):\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.lr = lr; self.n_epochs=n_epochs; self.batch_size=batch_size; self.verbose=verbose\n",
    "        self._init_weights()\n",
    "    def _init_weights(self):\n",
    "        rng = np.random.RandomState(123)\n",
    "        self.weights=[]; self.biases=[]\n",
    "        for i in range(len(self.layer_sizes)-1):\n",
    "            in_dim=self.layer_sizes[i]; out_dim=self.layer_sizes[i+1]\n",
    "            W = rng.normal(scale=np.sqrt(2.0/(in_dim+out_dim)), size=(in_dim,out_dim))\n",
    "            b = np.zeros(out_dim)\n",
    "            self.weights.append(W); self.biases.append(b)\n",
    "    def fit(self, X, y):\n",
    "        n = X.shape[0]\n",
    "        # build one-hot\n",
    "        y_enc = np.zeros((n,2))\n",
    "        y_enc[np.arange(n), y] = 1\n",
    "        for epoch in range(self.n_epochs):\n",
    "            perm = np.random.permutation(n)\n",
    "            X_sh = X[perm]; y_sh = y_enc[perm]\n",
    "            for i in range(0, n, self.batch_size):\n",
    "                xb = X_sh[i:i+self.batch_size]; yb = y_sh[i:i+self.batch_size]\n",
    "                activations = [xb]; pre_acts=[]\n",
    "                for W,b in zip(self.weights[:-1], self.biases[:-1]):\n",
    "                    z = activations[-1].dot(W) + b\n",
    "                    pre_acts.append(z)\n",
    "                    activations.append(relu(z))\n",
    "                z = activations[-1].dot(self.weights[-1]) + self.biases[-1]\n",
    "                pre_acts.append(z)\n",
    "                probs = softmax(z)\n",
    "                activations.append(probs)\n",
    "                delta = (probs - yb) / max(1, xb.shape[0])\n",
    "                grads_w=[]; grads_b=[]\n",
    "                # last layer grads\n",
    "                a_prev = activations[-2]\n",
    "                gw = a_prev.T.dot(delta); gb = delta.sum(axis=0)\n",
    "                grads_w.append(gw); grads_b.append(gb)\n",
    "                # backprop for remaining layers\n",
    "                delta = delta.dot(self.weights[-1].T) * drelu(pre_acts[-2])\n",
    "                for l in range(len(self.weights)-2, -1, -1):\n",
    "                    a_prev = activations[l]\n",
    "                    gw = a_prev.T.dot(delta); gb = delta.sum(axis=0)\n",
    "                    grads_w.append(gw); grads_b.append(gb)\n",
    "                    if l > 0:\n",
    "                        delta = delta.dot(self.weights[l].T) * drelu(pre_acts[l-1])\n",
    "                grads_w = grads_w[::-1]; grads_b = grads_b[::-1]\n",
    "                for idx in range(len(self.weights)):\n",
    "                    self.weights[idx] -= self.lr * grads_w[idx]\n",
    "                    self.biases[idx] -= self.lr * grads_b[idx]\n",
    "            if self.verbose and epoch % (self.n_epochs//5 + 1) == 0:\n",
    "                preds = self.predict(X)\n",
    "                acc = (preds == y).mean()\n",
    "                print(f\"[MLP] epoch {epoch} acc {acc:.4f}\")\n",
    "        return self\n",
    "    def predict_proba(self, X):\n",
    "        a = X\n",
    "        for W,b in zip(self.weights[:-1], self.biases[:-1]):\n",
    "            a = relu(a.dot(W) + b)\n",
    "        logits = a.dot(self.weights[-1]) + self.biases[-1]\n",
    "        return softmax(logits)\n",
    "    def predict(self, X):\n",
    "        return np.argmax(self.predict_proba(X), axis=1)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e06abd73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN\n",
    "class Conv1D_Simple:\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, lr=0.005):\n",
    "        # in_channels should be 1 for our tabular shape (features,1)\n",
    "        self.kernel_size = kernel_size\n",
    "        self.out_channels = out_channels\n",
    "        # kernel shape (out_channels, kernel_size)\n",
    "        rng = np.random.RandomState(123)\n",
    "        self.W = rng.normal(scale=0.1, size=(out_channels, kernel_size))\n",
    "        self.b = np.zeros(out_channels)\n",
    "        self.lr = lr\n",
    "    def forward(self, X):\n",
    "        # X shape (N, L, 1) -> treat last dim as channel 1\n",
    "        N, L, C = X.shape\n",
    "        K = self.kernel_size\n",
    "        outL = L - K + 1\n",
    "        Y = np.zeros((N, outL, self.out_channels))\n",
    "        for n in range(N):\n",
    "            for o in range(self.out_channels):\n",
    "                for i in range(outL):\n",
    "                    segment = X[n, i:i+K, 0]  # shape (K,)\n",
    "                    Y[n, i, o] = np.dot(self.W[o], segment) + self.b[o]\n",
    "        self.cache = (X, Y)\n",
    "        return Y\n",
    "    def backward(self, dY):\n",
    "        X, Y = self.cache\n",
    "        N, L, C = X.shape\n",
    "        K = self.kernel_size\n",
    "        outL = L - K + 1\n",
    "        dW = np.zeros_like(self.W)\n",
    "        db = np.zeros_like(self.b)\n",
    "        dX = np.zeros_like(X)\n",
    "        for n in range(N):\n",
    "            for o in range(self.out_channels):\n",
    "                for i in range(outL):\n",
    "                    seg = X[n, i:i+K, 0]\n",
    "                    dW[o] += dY[n, i, o] * seg\n",
    "                    db[o] += dY[n, i, o]\n",
    "                    dX[n, i:i+K, 0] += dY[n, i, o] * self.W[o]\n",
    "        # update\n",
    "        self.W -= self.lr * dW / N\n",
    "        self.b -= self.lr * db / N\n",
    "        return dX\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "39e10754",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_hybrid_rf_cnn_numPy(X_train, X_test, y_train, y_test, rf_model,\n",
    "                              conv_filters=16, kernel_size=5, conv_lr=0.005, dense_lr=0.001, epochs=60):\n",
    "    # Fit RF\n",
    "    rf_model.fit(X_train, y_train)\n",
    "    rf_train_probs = rf_model.predict_proba(X_train)[:,1].reshape(-1,1)\n",
    "    rf_test_probs = rf_model.predict_proba(X_test)[:,1].reshape(-1,1)\n",
    "    \n",
    "    X_train_cnn = np.hstack([X_train, rf_train_probs])[:,:,np.newaxis]\n",
    "    X_test_cnn  = np.hstack([X_test, rf_test_probs])[:,:,np.newaxis]\n",
    "    N, L, _ = X_train_cnn.shape\n",
    "\n",
    "    # Conv layer\n",
    "    conv = Conv1D_Simple(in_channels=1, out_channels=conv_filters, kernel_size=kernel_size, lr=conv_lr)\n",
    "\n",
    "    outL = L - kernel_size + 1\n",
    "    hidden_dim = outL * conv_filters\n",
    "\n",
    "    rng = np.random.RandomState(123)\n",
    "    W1 = rng.normal(scale=0.1, size=(hidden_dim, 64))\n",
    "    b1 = np.zeros(64)\n",
    "    W2 = rng.normal(scale=0.1, size=(64, 1))\n",
    "    b2 = np.zeros(1)\n",
    "\n",
    "    def sigmoid(x): return 1/(1+np.exp(-x))\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        conv_out = conv.forward(X_train_cnn)\n",
    "        flat = conv_out.reshape(N, -1)\n",
    "        z1 = flat.dot(W1) + b1\n",
    "        a1 = np.maximum(0, z1)\n",
    "        logits = a1.dot(W2) + b2\n",
    "        probs = sigmoid(logits).reshape(-1)\n",
    "        loss = -np.mean(y_train*np.log(probs+1e-12) + (1-y_train)*np.log(1-probs+1e-12))\n",
    "        \n",
    "        # Backprop dense\n",
    "        dlogits = (probs - y_train).reshape(-1,1)/N\n",
    "        dW2 = a1.T.dot(dlogits)\n",
    "        db2 = dlogits.sum(axis=0)\n",
    "        da1 = dlogits.dot(W2.T)\n",
    "        dz1 = da1 * (z1>0)\n",
    "        dW1 = flat.T.dot(dz1)\n",
    "        db1 = dz1.sum(axis=0)\n",
    "        dflat = dz1.dot(W1.T)\n",
    "        # Update dense\n",
    "        W2 -= dense_lr * dW2\n",
    "        b2 -= dense_lr * db2\n",
    "        W1 -= dense_lr * dW1\n",
    "        b1 -= dense_lr * db1\n",
    "        # Backprop into conv\n",
    "        dconv = dflat.reshape(conv_out.shape)\n",
    "        conv.backward(dconv)\n",
    "        \n",
    "        if epoch % max(1, epochs//5)==0:\n",
    "            preds = (probs>=0.5).astype(int)\n",
    "            acc = (preds==y_train).mean()\n",
    "            print(f\"[HybridRF+CNN] epoch {epoch} loss {loss:.4f} train acc {acc:.4f}\")\n",
    "\n",
    "    # Evaluate\n",
    "    conv_out_test = conv.forward(X_test_cnn)\n",
    "    flat_test = conv_out_test.reshape(X_test_cnn.shape[0], -1)\n",
    "    a1_test = np.maximum(0, flat_test.dot(W1) + b1)\n",
    "    logits_test = a1_test.dot(W2) + b2\n",
    "    probs_test = sigmoid(logits_test).reshape(-1)\n",
    "    acc_test = ((probs_test>=0.5).astype(int) == y_test).mean()\n",
    "    \n",
    "    return acc_test*100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "dd27272f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_hybrid_gb_dnn(X_train, X_test, y_train, y_test, gb_model=None,\n",
    "                        mlp_layers=None, mlp_lr=0.005, mlp_epochs=300):\n",
    "    # Increase n_estimators and reduce learning rate for GB\n",
    "    if gb_model is None:\n",
    "        gb_model = GradientBoostingStumps(n_estimators=300, learning_rate=0.05)\n",
    "    gb_model.fit(X_train, y_train)\n",
    "    \n",
    "    gb_probs_train = gb_model.predict_proba(X_train)[:,1].reshape(-1,1)\n",
    "    gb_probs_test  = gb_model.predict_proba(X_test)[:,1].reshape(-1,1)\n",
    "    X_train_hybrid = np.hstack([X_train, gb_probs_train])\n",
    "    X_test_hybrid  = np.hstack([X_test, gb_probs_test])\n",
    "    \n",
    "    if mlp_layers is None:\n",
    "        mlp_layers = [X_train_hybrid.shape[1], 256, 128, 64, 2]  # deeper network\n",
    "    \n",
    "    mlp = MLP_Numpy(\n",
    "        mlp_layers,\n",
    "        lr=mlp_lr,\n",
    "        n_epochs=mlp_epochs,\n",
    "        batch_size=32,  # smaller batch helps convergence\n",
    "        verbose=True\n",
    "    )\n",
    "    mlp.fit(X_train_hybrid, y_train)\n",
    "    preds = mlp.predict(X_test_hybrid)\n",
    "    acc = (preds == y_test).mean()\n",
    "    return acc*100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5d7639f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset: (2666, 11)\n",
      "[MLP] epoch 0 acc 0.5241\n",
      "[MLP] epoch 101 acc 0.8659\n",
      "[MLP] epoch 202 acc 0.8889\n",
      "[MLP] epoch 303 acc 0.8940\n",
      "[MLP] epoch 404 acc 0.9025\n",
      "[HybridRF+CNN] epoch 0 loss 0.7114 train acc 0.3812\n",
      "[HybridRF+CNN] epoch 12 loss 0.7107 train acc 0.3835\n",
      "[HybridRF+CNN] epoch 24 loss 0.7101 train acc 0.3872\n",
      "[HybridRF+CNN] epoch 36 loss 0.7094 train acc 0.3915\n",
      "[HybridRF+CNN] epoch 48 loss 0.7087 train acc 0.3910\n",
      "[MLP] epoch 0 acc 0.6128\n",
      "[MLP] epoch 61 acc 0.9006\n",
      "[MLP] epoch 122 acc 0.9255\n",
      "[MLP] epoch 183 acc 0.9541\n",
      "[MLP] epoch 244 acc 0.9700\n"
     ]
    }
   ],
   "source": [
    "# Train everything and print accuracies\n",
    "os.makedirs('models', exist_ok=True)\n",
    "df = generate_or_load_dataset(n_samples=2000)   # tweak n_samples if desired\n",
    "X = df.iloc[:,:-1].values.astype(float)\n",
    "y = df.iloc[:,-1].values.astype(int)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split_np(X,y,test_size=0.2,seed=42)\n",
    "scaler = StandardScalerManual()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "scaler.save('models/scaler_manual_numPy.joblib')\n",
    "\n",
    "scores = {}\n",
    "\n",
    "# Logistic Regression (from scratch)\n",
    "lr = LogisticRegressionSGD(lr=0.02, n_iter=2000)\n",
    "lr.fit(X_train_scaled, y_train)\n",
    "scores['Logistic Regression'] = round((lr.predict(X_test_scaled) == y_test).mean()*100, 2)\n",
    "\n",
    "# Linear SVM (from scratch)\n",
    "svm = LinearSVM_SGD(lr=0.005, n_iter=2000, C=1.0)\n",
    "svm.fit(X_train_scaled, y_train)\n",
    "scores['SVM'] = round((svm.predict(X_test_scaled) == y_test).mean()*100, 2)\n",
    "\n",
    "# Random Forest (from scratch)\n",
    "rf = RandomForest(\n",
    "    n_estimators=100, \n",
    "    max_depth=12, \n",
    "    min_samples_split=5, \n",
    "    max_features=int(np.sqrt(X.shape[1])), \n",
    "    seed=42\n",
    ")\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "scores['Random Forest'] = round((rf.predict(X_test_scaled) == y_test).mean()*100, 2)\n",
    "\n",
    "# Neural Network (from scratch)\n",
    "mlp = MLP_Numpy(\n",
    "    [X_train_scaled.shape[1], 128, 64, 2], \n",
    "    lr=0.005, \n",
    "    n_epochs=500, \n",
    "    batch_size=42, \n",
    "    verbose=True\n",
    ")\n",
    "mlp.fit(X_train_scaled, y_train)\n",
    "scores['Neural Network'] = round((mlp.predict(X_test_scaled) == y_test).mean()*100, 2)\n",
    "\n",
    "# Hybrid RF + CNN (from scratch CNN)\n",
    "rf_for_hybrid = RandomForest(\n",
    "    n_estimators=100, \n",
    "    max_depth=12, \n",
    "    min_samples_split=5, \n",
    "    max_features=int(np.sqrt(X.shape[1])), \n",
    "    seed=42\n",
    ")\n",
    "acc_rf_cnn = train_hybrid_rf_cnn_numPy(\n",
    "    X_train_scaled, X_test_scaled, y_train, y_test, rf_for_hybrid,\n",
    "    conv_filters=16, kernel_size=5, conv_lr=0.005, dense_lr=0.001, epochs=60\n",
    ")\n",
    "scores['Hybrid RF + CNN'] = round(acc_rf_cnn, 2)\n",
    "\n",
    "# Hybrid GB + DNN (from scratch)\n",
    "acc_gb_dnn = train_hybrid_gb_dnn(\n",
    "    X_train_scaled, X_test_scaled, y_train, y_test,\n",
    "    gb_model=GradientBoostingStumps(n_estimators=300, learning_rate=0.05),\n",
    "    mlp_layers=[X_train_scaled.shape[1]+1, 256, 128, 64, 2],\n",
    "    mlp_lr=0.005, mlp_epochs=300\n",
    ")\n",
    "scores['Hybrid GB + DNN'] = round(acc_gb_dnn, 2)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sign",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
